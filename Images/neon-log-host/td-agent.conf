#------------------------------------------------------------------------------
# Configures [neon-log-host] containers that will be deployed to every 
# hive node with the responsibility to receive local log events from the:
#
#       * Node's TD-Agent service which captures journald events
#       * Docker container events forwarded by the Fluentd plugin
#       * Container events transmitted as Syslog packets for containers
#         that don't support Docker logging (like HAProxy).
#
# The agent then adds hive and node fields to describe the datacenter, 
# environment, node, etc. and then forwards the events on to the hive 
# [neon-log-collector] log aggregator service.
#
# See the [neon-log-collector] image's [td-agent.conf] configuration file
# for more details.
#
# NOTE: This configuration depends on several environment variables having
#       been loaded from [/etc/neoncloud/host-env] by the container 
#       entrypoint script.

<source>    
    @type   forward
    bind    127.0.0.1
    port    24224
</source>

<source>
    @type   syslog
    bind    0.0.0.0
    port    "#{ENV['HiveHostPorts_LogHostSysLog']}"
    format  none
    tag     syslog
</source>

# Read events from the systemd journal.  Here's the documentation:
#
#   https://github.com/reevoo/fluent-plugin-systemd/blob/master/README.md

<source>
    @type               systemd
    path                /hostfs/var/log/journal
    pos_file            /hostfs/var/log/neon-log-host/journal.pos
    tag                 systemd
    read_from_head      true
    strip_underscores   true

    # We're going to process events from all systemd units by default
    # but you can uncomment and edit the line below to change this.
    #
    # filters           [{ "_SYSTEMD_UNIT": "docker.service" }, { "_SYSTEMD_UNIT": "consul.service" }]

</source>

# We need to avoid capturing logs from Fluentd/TD-Agent itself to fend off 
# cascading events when there's a problem with the log pipeline.  Operators
# will need to examine the source logs to diagnose these problems.

<match fluent.**>
    @type   null
</match>

# Add Neon hive and node information to all records.

<filter **>
    @type   record_transformer

    <record>
        cluster         "#{ENV['NEON_HIVE']}"
        datacenter      "#{ENV['NEON_DATACENTER']}"
        environment     "#{ENV['NEON_ENVIRONMENT']}"
        node            "#{ENV['NEON_NODE_NAME']}"
        node_ip         "#{ENV['NEON_NODE_IP']}"
        node_role       "#{ENV['NEON_NODE_ROLE']}"
    </record>
</filter>

# Forward all records including all raw fields to the downstream
# [neon-log-collector] service for normalization and storage.

<match **>
    @type               forward
    require_ack_response

    <buffer>
        @type               memory
        chunk_limit_size    8mb     # Maximum bytes for each buffered chuck
        total_limit_size    64mb    # Maximum bytes for all buffered chunks
        compress            text    # "text" means no compression, "gzip" to compress
        flush_at_shutdown   true
        flush_mode          interval
        flush_interval      1s
        flush_thread_count  2
        overflow_action     drop_oldest_chunk
        retry_type          exponential_backoff
        retry_wait          1s
        retry_max_interval  15s
    </buffer>

    # Note that the [neon-log-collector] service can be reached via
    # the hive's private proxy via the TCP route on 
    # HiveHostPorts_ProxyPrivateTcpLogCollector].
    #
    # Note also that we need to use a TCP heartbeat because the proxy
    # can't route UDP packets.

    heartbeat_type      tcp

    <server>
        name    neon-log-collector
        host    127.0.0.1
        port    "#{ENV['HiveHostPorts_ProxyPrivateTcpLogCollector']}"
    </server>
</match>
